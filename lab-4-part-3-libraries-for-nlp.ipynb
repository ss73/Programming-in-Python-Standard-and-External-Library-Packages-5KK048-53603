{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc9f145",
   "metadata": {
    "papermill": {
     "duration": 0.007492,
     "end_time": "2024-10-02T15:55:06.436868",
     "exception": false,
     "start_time": "2024-10-02T15:55:06.429376",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d38e27",
   "metadata": {
    "papermill": {
     "duration": 0.006764,
     "end_time": "2024-10-02T15:55:06.451148",
     "exception": false,
     "start_time": "2024-10-02T15:55:06.444384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Libraries for natural language processing (NLP)\n",
    "================================================\n",
    "\n",
    "In this lab, you will train an unsupervised language model on a corpus. You will also use a pre-trained language model.\n",
    "\n",
    "In this lab, we will use the external library: [Gensim](https://radimrehurek.com/gensim/).\n",
    "\n",
    "In this lab, you will also practice using default and keyword arguments to functions, as well as using klasses, instance variables and methods.\n",
    "\n",
    "The questions marked in bold might appear in the quizz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633ea15",
   "metadata": {
    "papermill": {
     "duration": 0.006856,
     "end_time": "2024-10-02T15:55:06.465242",
     "exception": false,
     "start_time": "2024-10-02T15:55:06.458386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I. Importing modules and defining functions\n",
    "----------------------------------------\n",
    "First, the modules, classes and functions needed are imported.\n",
    "Then two functions are defined.\n",
    "(Nothing happens when you run this code block. But if running individual code blocks below, you need to this block first, to have everything imported and defined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bca225",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-02T15:55:06.481531Z",
     "iopub.status.busy": "2024-10-02T15:55:06.481074Z",
     "iopub.status.idle": "2024-10-02T15:55:21.756075Z",
     "shell.execute_reply": "2024-10-02T15:55:21.754869Z"
    },
    "papermill": {
     "duration": 15.286566,
     "end_time": "2024-10-02T15:55:21.758876",
     "exception": false,
     "start_time": "2024-10-02T15:55:06.472310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Import a class from gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Import another a class from gensim.models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Import a sub-module (thereby, when you use the function 'simple_preprocess' \n",
    "# from this sub-module, \n",
    "# you need to to prefix it with 'utils'\n",
    "from gensim import utils\n",
    "\n",
    "KEYED_VECTORS_NAME = \"newsgroups.kv\"\n",
    "\n",
    "def train_model_on_documents_list(document_list, window=5, min_count=5, nr_of_documents=None):\n",
    "    \n",
    "    if nr_of_documents:\n",
    "        documents_to_use =  document_list[:nr_of_documents]\n",
    "    else:\n",
    "        documents_to_use = document_list\n",
    "        \n",
    "    # A simple built-in utility function that does a white-space-based tokenization\n",
    "    # (Need to prefix 'simple_preprocess' with 'utils', since only 'utils' is imported)\n",
    "    token_lists = [utils.simple_preprocess(document) for document in documents_to_use]\n",
    "    \n",
    "    # Create an instance of the a Word2vec class, which also trains a model\n",
    "    # (The class Word2Vec is imported, so no prefixes are needed)\n",
    "    model = Word2Vec(sentences=token_lists, window=window, min_count=min_count, workers = 4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_similar_words(word, model_vectors, topn=10):\n",
    "    if word in model_vectors:\n",
    "        return model_vectors.most_similar(word, topn=topn)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_and_preprocess_news_group_texts():\n",
    "    data = []\n",
    "    \n",
    "    for j in glob.glob(f'/kaggle/input/news-groups-lab/20news-18828/*/*'):\n",
    "        with open(j, 'r', encoding='cp1252') as f:\n",
    "            lines = [line.replace(\"_\", \" \").strip() for line in f.readlines() if not (line.startswith(\"From:\") or line.startswith(\"Subject:\"))]\n",
    "            data.append(\" \".join(lines))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c123438",
   "metadata": {
    "papermill": {
     "duration": 0.006771,
     "end_time": "2024-10-02T15:55:21.772906",
     "exception": false,
     "start_time": "2024-10-02T15:55:21.766135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**1) ❓💬❓There is a Python naming convention for classes, and another naming convention for functions and methods. Can you see the different naming conventions in the code above? (We will not talk much about object orientation in this course. But to use external libraries, you need to be able to use classes, function and methods.)**\n",
    "\n",
    "> !💬! The naming convention for classes is *camel case*, i.e. start each word in capital letters, eliminate spaces, e.g. ```CamelCase```. The naming convention for methods is *snake case*, i.e. using only small letters, separate words with underscore characters, e.g. ```snake_case```. Examples of classes above are *Word2Vec* and *KeyedVectors*, examples of methods are *most_similar*, *replace*, *strip*.\n",
    "\n",
    "**2) ❓💬❓Which arguments for the two functions above have default values?**\n",
    "\n",
    "> !💬! The function ```train_model_on_documents_list``` has default values for arguments ```window```, ```min_count``` and ```nr_of_documents```.<br>\n",
    "> The function ```get_similar_words``` has default values for only one argument; ```topn```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8b80e",
   "metadata": {
    "papermill": {
     "duration": 0.006982,
     "end_time": "2024-10-02T15:55:21.787731",
     "exception": false,
     "start_time": "2024-10-02T15:55:21.780749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "II. Import a text corpus and train a word2vec model\n",
    "----------------------------------------------------\n",
    "Below, you import a text corpus, using scikit learn. Then you use this text corpus to train a word2vec model. Finally, the model is saved. (See \"Output\" to the right, i.e. /kaggle/working.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24271d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:55:21.805086Z",
     "iopub.status.busy": "2024-10-02T15:55:21.804244Z",
     "iopub.status.idle": "2024-10-02T15:56:19.619643Z",
     "shell.execute_reply": "2024-10-02T15:56:19.618251Z"
    },
    "papermill": {
     "duration": 57.833977,
     "end_time": "2024-10-02T15:56:19.629427",
     "exception": false,
     "start_time": "2024-10-02T15:55:21.795450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have created an instance of: <class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "newsgroups_data = sorted(get_and_preprocess_news_group_texts())\n",
    "\n",
    "# The function train_model_on_documents_list creates an instance of the class Word2Vec\n",
    "model = train_model_on_documents_list(newsgroups_data, nr_of_documents=1000)\n",
    "\n",
    "print(\"We have created an instance of:\", type(model))\n",
    "trained_vectors = model.wv\n",
    "\n",
    "# Saving the the keyed-vectors instance, rather than saving the model takes up less space\n",
    "# The drawback is that you can't continue training it\n",
    "trained_vectors.save(KEYED_VECTORS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b89638",
   "metadata": {
    "papermill": {
     "duration": 0.007432,
     "end_time": "2024-10-02T15:56:19.644462",
     "exception": false,
     "start_time": "2024-10-02T15:56:19.637030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**3) ❓💬❓ Two Python [built-in](https://docs.python.org/3/library/functions.html) functions are used. Can you see where?** How is it colour-coded in Kaggle?\n",
    "\n",
    "> !💬! Actually, *three* built functions are used: ```sorted```, ```print``` and ```type```. Kaggle color-codes them green\n",
    "\n",
    "**4) ❓💬❓Our own defined function `train_model_on_documents_list` returns an instance of the gensim [Word2Vec-class](https://radimrehurek.com/gensim/models/word2vec.html). `model.wv` is an instance-variable that stores a KeyedVectors instance. When you run `trained_vectors.save(KEYED_VECTORS_NAME)`, you use one of the instance-methods of KeyedVector. Can you spot the syntax difference bewteen an instance-variable and an instande-method?**\n",
    "\n",
    "> !💬! The instance method is ```save```, and the difference in syntax compared to an instance variable is that an instance method is always called with arguments inside parenthesis, even if there are zero arguments, i.e. ```save()```. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18fc03a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:56:19.663042Z",
     "iopub.status.busy": "2024-10-02T15:56:19.662375Z",
     "iopub.status.idle": "2024-10-02T15:56:19.691062Z",
     "shell.execute_reply": "2024-10-02T15:56:19.689430Z"
    },
    "papermill": {
     "duration": 0.044124,
     "end_time": "2024-10-02T15:56:19.696390",
     "exception": false,
     "start_time": "2024-10-02T15:56:19.652266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have loaded an instance of the class: <class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "\n",
      "The total number of word-vectors in the model is: 6747\n",
      "\n",
      "The first five elements in the first vector look like this: \n",
      " [-0.19201891 -0.48475322 -0.5871777   2.461023   -1.4539908 ]\n",
      "The first five elements in the second vector look like this: \n",
      " [-0.6393317  -0.24812081 -0.43280372  0.99472964 -0.22260278]\n",
      "\n",
      "computer [('international', 0.9925001263618469), ('relativity', 0.9911588430404663), ('bodies', 0.9906104803085327), ('seattle', 0.9900129437446594), ('seven', 0.9898717403411865), ('security', 0.9890558123588562), ('persons', 0.9889115691184998), ('various', 0.9885237812995911), ('rate', 0.9882726073265076), ('institutes', 0.9881563186645508)]\n",
      "jump [('hardware', 0.9903427958488464), ('monitors', 0.9898173809051514), ('avoid', 0.9886245131492615), ('murder', 0.9884785413742065), ('union', 0.9881682395935059), ('scanner', 0.9880817532539368), ('grant', 0.9876208901405334), ('alone', 0.9872841835021973), ('utilities', 0.986893355846405), ('funding', 0.9864829778671265)]\n",
      "dance None\n",
      "bike [('hockey', 0.9939298033714294), ('slow', 0.9938217997550964), ('color', 0.9931492209434509), ('returned', 0.9926755428314209), ('drug', 0.9924907684326172), ('format', 0.9924402236938477), ('short', 0.9923970699310303), ('detailed', 0.9923279285430908), ('capacity', 0.9922075271606445), ('credit', 0.9920563697814941)]\n",
      "good [('even', 0.9930406808853149), ('too', 0.9920511841773987), ('something', 0.9895066618919373), ('where', 0.9885520935058594), ('little', 0.9880495667457581), ('now', 0.9869028329849243), ('really', 0.986767590045929), ('still', 0.9861568212509155), ('asked', 0.9856224656105042), ('nothing', 0.9840554594993591)]\n",
      "tea [('distributed', 0.962787926197052), ('directly', 0.9622887372970581), ('shops', 0.9608418345451355), ('items', 0.960724949836731), ('constant', 0.9604915976524353), ('network', 0.96004319190979), ('denied', 0.9600139260292053), ('belt', 0.9598337411880493), ('stopped', 0.9598212242126465), ('collective', 0.9596314430236816)]\n",
      "coffe None\n",
      "sing None\n",
      "juice [('vga', 0.9926460981369019), ('begins', 0.9907384514808655), ('laboratories', 0.9905362725257874), ('clip', 0.9902331829071045), ('playoff', 0.9901943802833557), ('atoms', 0.9901784658432007), ('lack', 0.9901404976844788), ('wall', 0.9899489283561707), ('hiv', 0.9898581504821777), ('vi', 0.9896817207336426)]\n"
     ]
    }
   ],
   "source": [
    "# Load an instance of KeyedVectors\n",
    "loaded_word2vec_vectors = KeyedVectors.load(KEYED_VECTORS_NAME)\n",
    "print(\"You have loaded an instance of the class:\", type(loaded_word2vec_vectors))\n",
    "\n",
    "# Print some statistics for the KeyedVector instance that you loaded\n",
    "print(\"\\nThe total number of word-vectors in the model is:\", len(loaded_word2vec_vectors))\n",
    "print(\"\\nThe first five elements in the first vector look like this: \\n\", \n",
    "      loaded_word2vec_vectors[0][:5])\n",
    "print(\"The first five elements in the second vector look like this: \\n\", \n",
    "      loaded_word2vec_vectors[1][:5])\n",
    "\n",
    "\n",
    "print()\n",
    "for word in [\"computer\", \"jump\", \"dance\", \"bike\", \"good\", \"tea\", \"coffe\", \"sing\", \"juice\"]:\n",
    "    print(word, get_similar_words(word, loaded_word2vec_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0771d259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:56:19.730599Z",
     "iopub.status.busy": "2024-10-02T15:56:19.728553Z",
     "iopub.status.idle": "2024-10-02T15:56:57.138915Z",
     "shell.execute_reply": "2024-10-02T15:56:57.128288Z"
    },
    "papermill": {
     "duration": 37.440074,
     "end_time": "2024-10-02T15:56:57.151902",
     "exception": false,
     "start_time": "2024-10-02T15:56:19.711828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer : lab:0.65, architecture:0.63, engineering:0.62, network:0.61, silicon:0.61, software:0.6, computing:0.6, multimedia:0.59, technology:0.59, systems:0.59 \n",
      "\n",
      "jump : hang:0.77, turn:0.74, sit:0.72, walk:0.72, break:0.69, move:0.68, pass:0.67, pull:0.67, drop:0.67, kick:0.66 \n",
      "\n",
      "dance : panasonic:0.69, vhs:0.67, zyxel:0.66, strap:0.66, cassettes:0.66, steel:0.65, disc:0.65, broadway:0.65, xga:0.65, suns:0.64 \n",
      "\n",
      "bike : riding:0.78, car:0.74, ride:0.74, tires:0.73, seat:0.73, helmet:0.72, tire:0.7, truck:0.68, cage:0.68, passenger:0.66 \n",
      "\n",
      "good : bad:0.75, great:0.63, fair:0.61, nice:0.61, reasonable:0.58, poor:0.58, best:0.57, decent:0.57, big:0.56, perfect:0.53 \n",
      "\n",
      "tea : cups:0.69, goaltenders:0.69, portugal:0.65, crunching:0.64, beers:0.64, skates:0.63, jacket:0.63, shorted:0.63, corn:0.63, seater:0.62 \n",
      "\n",
      "coffe : - \n",
      "\n",
      "sing : crush:0.78, cain:0.76, forsake:0.75, kaan:0.75, eph:0.73, bury:0.73, persist:0.73, chorus:0.72, wreck:0.71, bribe:0.7 \n",
      "\n",
      "juice : flu:0.77, glasses:0.76, sixteen:0.76, jaws:0.75, swelling:0.74, vinegar:0.74, bricks:0.74, elbow:0.74, accelerating:0.73, tumors:0.73 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain model (task 9 below)\n",
    "model_retrained = train_model_on_documents_list(newsgroups_data, nr_of_documents=100000, window=4, min_count=10)\n",
    "for word in [\"computer\", \"jump\", \"dance\", \"bike\", \"good\", \"tea\", \"coffe\", \"sing\", \"juice\"]:\n",
    "    similar = get_similar_words(word, model_retrained.wv)\n",
    "    pretty = [x[0]+':'+str(round(x[1], 2)) for x in similar] if similar else '-'\n",
    "    print(word, ':', ', '.join(pretty) ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8ea096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:56:57.187315Z",
     "iopub.status.busy": "2024-10-02T15:56:57.186653Z",
     "iopub.status.idle": "2024-10-02T15:56:57.202136Z",
     "shell.execute_reply": "2024-10-02T15:56:57.200629Z"
    },
    "papermill": {
     "duration": 0.039942,
     "end_time": "2024-10-02T15:56:57.207972",
     "exception": false,
     "start_time": "2024-10-02T15:56:57.168030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 11 below\n",
    "len(model_retrained.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324800eb",
   "metadata": {
    "papermill": {
     "duration": 0.014781,
     "end_time": "2024-10-02T15:56:57.240016",
     "exception": false,
     "start_time": "2024-10-02T15:56:57.225235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**5) ❓💬❓When you load your saved KeyedVectors, you use a class-method. How do you see that it is a class-method?**\n",
    "\n",
    "> !💬! Class methods are called be prepending the class name, in this case ```KeyedVectors.load()```\n",
    "\n",
    "**6) ❓💬❓ In the code that trains a Word2Vec model and creates an instance of a Word2Vec model  `model = Word2Vec(sentences=token_lists, window=window, min_count=min_count)` three keyword arguments are used (and not positional arguments).**\n",
    "\n",
    "There are many other parameters to the instantiation of `Word2Vec`, and they get their default values. What are these parameters, and their default values? (See the documentation: [Word2Vec-class](https://radimrehurek.com/gensim/models/word2vec.html), search for `gensim.models.word2vec.Word2Vec`). You don't have to understand what the parameters do, it's just to practice reading documentation for external libraries.\n",
    "\n",
    "> !💬! Remaining parameters are: corpus_file, vector_size, alpha, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows\n",
    "\n",
    "**7) ❓💬❓Find out, either by reading the documentation for `gensim.models.word2vec.Word2Vec`, or by printing out the length of the vectors in the code above. What is the length of the vectors for the Word2vec model that has been trained?**\n",
    "\n",
    "> !💬! Vector length is specified by the named attribute ```vector_size```, and here the default value of **100** is applied.\n",
    "\n",
    "8) Take a look at the output of the model. How many words does the model contain? The semantically closest words to four given words are shown. Are they semantically close in your opinion?\n",
    "\n",
    "> !💬! The model contains **6747** words. The closeness is poor from a human interpretation\n",
    "\n",
    "9) 💻💻 Change the parameters that you use for envoking the `train_model_on_documents_list` function. Now default parameters are used for some arguments. Instead use a window size of *4*, a minimum document occurrence (`min_count`) to *10*, and set the nr of documents to use for training to *100000*.\n",
    "\n",
    "10) With the new settings? How is the quality of the semantically similar words? The same, or has it changed?\n",
    "\n",
    "> !💬! The semantic simlarity is much better in general, but still poor for some of the words\n",
    "\n",
    "**11) ❓💬❓ How many word-vectors does the model now contain?**\n",
    "\n",
    "> !💬! The model contains **22142** words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096899b",
   "metadata": {
    "papermill": {
     "duration": 0.007606,
     "end_time": "2024-10-02T15:56:57.258442",
     "exception": false,
     "start_time": "2024-10-02T15:56:57.250836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "III. Use a pre-trained word2vec model\n",
    "--------------------------------------\n",
    "Here, we will use a pre-trained model.\n",
    "\n",
    "It is trained on 1 million words and, and it has 300-dimensional vectors (i.e., that contain 300 elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a7a75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:56:57.276038Z",
     "iopub.status.busy": "2024-10-02T15:56:57.275601Z",
     "iopub.status.idle": "2024-10-02T15:58:24.715103Z",
     "shell.execute_reply": "2024-10-02T15:58:24.713479Z"
    },
    "papermill": {
     "duration": 87.453277,
     "end_time": "2024-10-02T15:58:24.719833",
     "exception": false,
     "start_time": "2024-10-02T15:56:57.266556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a pre-trained language model. Might take some time\n",
      "computer [('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426), ('technician_Leonard_Luchko', 0.5662748217582703), ('mainframes_minicomputers', 0.5617720484733582), ('laptop_computers', 0.5585449934005737), ('PC', 0.5539618730545044), ('maker_Dell_DELL.O', 0.5519254207611084)]\n",
      "jump [('jumps', 0.7040783166885376), ('jumping', 0.6920815706253052), ('leap', 0.6579699516296387), ('jumped', 0.5995128154754639), ('climb', 0.5455771088600159), ('leaping', 0.5435682535171509), ('Jump', 0.5267875790596008), ('drop', 0.5252555012702942), ('leaped', 0.5227952599525452), ('leapt', 0.5092256665229797)]\n",
      "dance [('dancing', 0.8380802869796753), ('dances', 0.8213121891021729), ('dancers', 0.7513905763626099), ('Dance', 0.739539384841919), ('ballroom_dance', 0.7040098309516907), ('dancer', 0.6916878819465637), ('dance_troupe', 0.6868269443511963), ('dance_routines', 0.6813735365867615), ('lindy_hop', 0.6773399114608765), ('ballet', 0.6756706833839417)]\n",
      "bike [('bicycle', 0.8521308898925781), ('bikes', 0.8127913475036621), ('mountain_bike', 0.7518467307090759), ('scooter', 0.749575674533844), ('motorcycle', 0.7016158699989319), ('Bike', 0.7006312012672424), ('biking', 0.679279625415802), ('BMX_bike', 0.6724931597709656), ('bicycles', 0.670872688293457), ('moped', 0.6429066061973572)]\n",
      "good [('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348341941833), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728850364685), ('solid', 0.5806034803390503), ('lousy', 0.576420247554779)]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the code below to run it\n",
    "print(\"Loading a pre-trained language model. Might take some time\")\n",
    "pre_trained_vectors = KeyedVectors.load_word2vec_format(\"/kaggle/input/google-news/GoogleNews-vectors-negative300.bin\", binary=True, unicode_errors='ignore')\n",
    "\n",
    "for word in [\"computer\", \"jump\", \"dance\", \"bike\", \"good\"]:\n",
    "    print(word, get_similar_words(word, pre_trained_vectors))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f1ddf",
   "metadata": {
    "papermill": {
     "duration": 0.016703,
     "end_time": "2024-10-02T15:58:24.753415",
     "exception": false,
     "start_time": "2024-10-02T15:58:24.736712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "12) With this larger model, how is the quality of the semantically similar words? The same, or has it changed?\n",
    "\n",
    "> !💬! The semantic simlarity generally makes better sense. The tokenization could probably be better as there are many variants on the same word returned as semantically similar. Not to say this is untrue, but it might not be that relevant.\n",
    "\n",
    "13) The number next to the neighbouring words is the cosine similarity between the vectors for the words. So, e.g., 'good' and 'great' has the cosine similarity '0.7291510105133057'. \n",
    "Read the documentation for [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) for how to use the instance-method `similarity(()` to calculate the distance between (i) the two words 'book' and 'novel', and the two words (ii) 'book' and 'mind'.  \n",
    "\n",
    "**14) ❓💬❓ What is the cosine similarity between 'book' and 'mind'** \n",
    "\n",
    "> !💬! Output from the code below: 0.13833922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe87707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:58:24.780876Z",
     "iopub.status.busy": "2024-10-02T15:58:24.780437Z",
     "iopub.status.idle": "2024-10-02T15:58:24.788383Z",
     "shell.execute_reply": "2024-10-02T15:58:24.787143Z"
    },
    "papermill": {
     "duration": 0.020769,
     "end_time": "2024-10-02T15:58:24.790929",
     "exception": false,
     "start_time": "2024-10-02T15:58:24.770160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book and newspaper similarity 0.22616692\n",
      "book and mind similarity 0.13833922\n",
      "Distances printed\n"
     ]
    }
   ],
   "source": [
    "# Distance between 'book' and 'newspaper':\n",
    "print('book and newspaper similarity', pre_trained_vectors.similarity('book', 'newspaper'))\n",
    "\n",
    "# Distance between 'book' and 'mind':\n",
    "print('book and mind similarity', pre_trained_vectors.similarity('book', 'mind'))\n",
    "\n",
    "print(\"Distances printed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08936a6",
   "metadata": {
    "papermill": {
     "duration": 0.007925,
     "end_time": "2024-10-02T15:58:24.807385",
     "exception": false,
     "start_time": "2024-10-02T15:58:24.799460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "IV: Find out which words are the most common in different news groups\n",
    "==========================================================================\n",
    "Here you will use classes from the machine learning library scikit [scikit-learn](https://scikit-learn.org/stable/index.html)\n",
    "And a stopword list from the [NLTK library](https://www.nltk.org)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fb5d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:58:24.825557Z",
     "iopub.status.busy": "2024-10-02T15:58:24.825131Z",
     "iopub.status.idle": "2024-10-02T15:58:41.765443Z",
     "shell.execute_reply": "2024-10-02T15:58:41.762055Z"
    },
    "papermill": {
     "duration": 16.965044,
     "end_time": "2024-10-02T15:58:41.780507",
     "exception": false,
     "start_time": "2024-10-02T15:58:24.815463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: sci.space\n",
      "Reading from: sci.med\n",
      "Reading from: sci.crypt\n",
      "Reading from: rec.sport.baseball\n",
      "Reading from: alt.atheism\n",
      "Have read texts for 5 news groups\n",
      "\n",
      "\n",
      "The 100 most typical words for SCI.SPACE are:\n",
      "[(0.20800392767693884, 'spacecraft'), (0.20073125569981626, 'orbit'), (0.18879931961143948, 'launch'), (0.17333660639744902, 'lunar'), (0.16275803414779264, 'shuttle'), (0.14946013978612827, 'solar'), (0.141721103421798, 'satellite'), (0.11945254557589449, 'mars'), (0.11916891689824621, 'hst'), (0.1072159433461687, 'venus'), (0.10400196383846942, 'zoo toronto'), (0.09964009319075218, 'sky'), (0.08259970233000476, 'flight'), (0.08157734819817183, 'missions'), (0.07583476529888396, 'henry zoo'), (0.07439029357890521, 'baalke'), (0.07341961337835466, 'orbital'), (0.07294582185892647, 'space station'), (0.07294582185892647, 'comet'), (0.072836918034082, 'vehicle'), (0.072836918034082, 'alaska edu'), (0.07150135013894772, 'sci space'), (0.07150135013894772, 'pluto'), (0.07013501705174305, 'satellites'), (0.07013501705174305, 'access digex'), (0.0696513272789724, 'zoo'), (0.06875805062417341, 'henry spencer'), (0.06789017083900087, 'orbiter'), (0.06716793497901151, 'probes'), (0.06551010874448654, 'astronomy'), (0.06481442955126598, 'alaska'), (0.06427899153905402, 'nsmca'), (0.06351379252571951, 'telescope'), (0.06347563331763913, 'atmosphere'), (0.06283451981907527, 'payload'), (0.06239598068741277, 'propulsion'), (0.06144115789079172, 'jpl'), (0.06142860114187149, 'planetary'), (0.06103426280542224, 'rocket'), (0.05940668246394431, 'sci'), (0.05899978737857483, 'digex'), (0.058501104659139054, 'prb access'), (0.058501104659139054, 'prb'), (0.058501104659139054, 'higgins'), (0.058501104659139054, 'aurora alaska'), (0.05777886879914968, 'rockets'), (0.05777886879914968, 'pat writes'), (0.05710414373872029, 'solar system'), (0.05416768949920282, 'observatory'), (0.05416768949920282, 'kilometers'), (0.05360797167308435, 'saturn'), (0.05360797167308435, 'planets'), (0.05330325618340209, 'jupiter'), (0.052442580984539036, 'space shuttle'), (0.051278746059245345, 'astronomical'), (0.0508618856711852, 'spencer'), (0.05030373636814673, 'launched'), (0.04838980261928785, 'orbiting'), (0.04788528750429353, 'jsc'), (0.04719982990285987, 'probe'), (0.046945330899309114, 'night sky'), (0.046945330899309114, 'kelvin jpl'), (0.045950528413210956, 'titan'), (0.04545023685326717, 'edu henry'), (0.044778623319341, 'voyager'), (0.044778623319341, 'toronto zoology'), (0.044778623319341, 'oort'), (0.044056387459351636, 'flyby'), (0.043334151599362256, 'utzoo henry'), (0.043334151599362256, 'ssto'), (0.043334151599362256, 'spencer toronto'), (0.043334151599362256, 'gamma ray'), (0.043334151599362256, 'edu utzoo'), (0.043334151599362256, 'billboard'), (0.043119455476176544, 'gamma'), (0.043119455476176544, 'engines'), (0.04208101023104582, 'exploration'), (0.04111363068550454, 'russian'), (0.04111363068550454, 'aurora'), (0.03946882328083971, 'cso uiuc'), (0.03946882328083971, 'cso'), (0.03904058806626796, 'larson'), (0.03900073643942603, 'astronaut'), (0.03869518182165133, 'digex net'), (0.0384578927219953, 'altitude'), (0.038278500579436665, 'ron baalke'), (0.03755626471944729, 'manned'), (0.03743434785399231, 'ti'), (0.03743434785399231, 'fuel'), (0.03743434785399231, 'fred'), (0.037292502033449984, 'utzoo'), (0.03724411250333941, 'cloud'), (0.03683402885945792, 'magellan'), (0.036760422730568766, 'dseg'), (0.03612711134490467, 'zoology'), (0.036111792999468546, 'centaur'), (0.036111792999468546, 'baalke kelvin'), (0.03554441600063202, 'launches'), (0.035399872427144895, 'astro'), (0.03538955713947918, 'net prb')]\n",
      "\n",
      "The 100 most typical words for SCI.MED are:\n",
      "[(0.21244170755119768, 'medical'), (0.1924207037546205, 'hiv'), (0.1521187535551786, 'cancer'), (0.15037026213500412, 'patients'), (0.13810840834000987, 'candida'), (0.11518089292996671, 'infection'), (0.10516516310996961, 'clinical'), (0.10391319688246997, 'yeast'), (0.10080677116549727, 'aids'), (0.09747839667472652, 'medicine'), (0.09543446194281582, 'gordon banks'), (0.09465857200832138, 'dyer'), (0.09092155384907226, '92'), (0.0891730624288978, 'diet'), (0.08873593957385419, 'vitamin'), (0.08767556259787145, 'geb'), (0.08612378272888256, 'medical newsletter'), (0.08469847268028895, 'symptoms'), (0.08457200285989369, 'newsletter page'), (0.08457200285989369, 'hicnet medical'), (0.08457200285989369, 'hicnet'), (0.0813048510381127, 'drug'), (0.07949985544622702, 'chronic'), (0.07526132364596044, '12 92'), (0.07474800821245846, 'patient'), (0.07430602204467067, 'pitt edu'), (0.07293365384247713, 'therapy'), (0.07222753191754702, 'med'), (0.0682783142355105, 'lyme'), (0.06760617628498046, 'physician'), (0.06635421005748082, 'foods'), (0.06439886456303832, 'quack'), (0.06259831137498191, 'infections'), (0.06250856827123719, 'doctors'), (0.062071194759555005, 'skepticism chastity'), (0.062071194759555005, 'chastity intellect'), (0.062071194759555005, 'banks skepticism'), (0.06072036203373245, 'surgery'), (0.06072036203373245, 'kidney'), (0.06051941489056612, 'surrender soon'), (0.06051941489056612, 'shameful surrender'), (0.06051941489056612, 'shameful'), (0.06051941489056612, 'intellect geb'), (0.06051941489056612, 'geb cadre'), (0.06051941489056612, 'edu shameful'), (0.06051941489056612, 'dsl pitt'), (0.06051941489056612, 'dsl'), (0.06051941489056612, 'cadre dsl'), (0.05975659115480508, 'diseases'), (0.05857446257584464, 'sci'), (0.058191745087082816, 'sci med'), (0.057158478495900514, 'volume number'), (0.05682597115567017, 'drugs'), (0.055864075283599504, 'prevention'), (0.05508818534910507, 'treatments'), (0.05446053089623426, 'nutrition'), (0.053328988315321235, 'syndrome'), (0.05320856466873462, 'liver'), (0.05276051554562175, 'diagnosed'), (0.05196225317809138, 'hospital'), (0.05114337404010315, 'women'), (0.05007864909998552, 'chastity'), (0.049656955807644, 'kirlian'), (0.049656955807644, 'infected'), (0.048826682872485885, 'cadre'), (0.04732928600416069, 'rind'), (0.04694873353123643, 'noring'), (0.04632275041748661, 'allergic'), (0.045777506135171814, 'placebo'), (0.04569676730373679, 'breast'), (0.04500161620067738, 'sinus'), (0.044444801076237155, 'photography'), (0.044225726266182944, 'systemic'), (0.044149408359405284, 'keyboard'), (0.0434498363316885, 'public health'), (0.0434498363316885, 'antibiotics'), (0.04319283484873751, 'spdcc'), (0.04319283484873751, 'physicians'), (0.04312867013781585, 'skepticism'), (0.04283803979427443, 'skin'), (0.04267394639719406, 'number 11'), (0.04267394639719406, 'chromium'), (0.04267394639719406, '25 1993'), (0.04267394639719406, '20 1993'), (0.04267394639719406, '11 april'), (0.041898056462699625, '10 april'), (0.041569802542473104, 'intellect'), (0.04068890239373824, 'cdc'), (0.040530557478911274, 'surrender'), (0.040346276593710755, 'vaginal'), (0.03957038665921632, 'steve dyer'), (0.03957038665921632, 'antibiotic'), (0.0393410569539255, 'tests'), (0.03881095305248878, 'diagnosis'), (0.03879449672472188, 'dose'), (0.03879449672472188, 'diabetes'), (0.03818496993873896, 'illness'), (0.038018606790227435, 'retinol'), (0.038018606790227435, 'corn'), (0.038018606790227435, '17 12')]\n",
      "\n",
      "The 100 most typical words for SCI.CRYPT are:\n",
      "[(0.47233836366301807, 'encryption'), (0.2541882548712138, 'clipper'), (0.21861307223386522, 'db'), (0.20285378000574758, 'escrow'), (0.16262759609445668, 'privacy'), (0.15793968272953585, 'clipper chip'), (0.13684517232338428, 'des'), (0.13572941484569484, 'law enforcement'), (0.1353887879043974, 'pgp'), (0.13387027727287593, 'nsa'), (0.13375516881157565, 'encrypted'), (0.11070024422771316, 'crypto'), (0.10759640885949628, 'eff'), (0.09648818877048505, 'secure'), (0.09477215153307819, 'public key'), (0.09158653299415119, 'cryptography'), (0.0853861409756553, 'ripem'), (0.08402068896419956, 'rsa'), (0.08291833343300631, 'sternlight'), (0.07757984053880498, 'enforcement'), (0.07699559533064872, 'david sternlight'), (0.07685304725161382, 'chips'), (0.07486203566478444, 'wiretap'), (0.07057929571976132, 'key escrow'), (0.06761792666858252, 'mov'), (0.06218875007475473, 'cipher'), (0.061323156874344704, 'serial number'), (0.06021450404063554, 'denning'), (0.0595056841408755, 'bits'), (0.05839342836254138, 'agencies'), (0.057341133700685956, 'fbi'), (0.05577245046386734, 'plaintext'), (0.0539444052492049, 'administration'), (0.05057169430546608, 'crypt'), (0.050242671964141096, 'nist'), (0.04921731819128488, 'serial'), (0.04442053576768195, 'cryptographic'), (0.04293985124209255, 'nist gov'), (0.04220944564078272, 's2'), (0.041709591687529565, 'scheme'), (0.041709591687529565, 'phones'), (0.0410148386886851, 'anonymity'), (0.04065689902361417, 'classified'), (0.04061663637131922, 'criminals'), (0.04047204369944356, 'ncsl'), (0.03982023173658747, 'sci crypt'), (0.0394220294192216, 's1 s2'), (0.03933472344561046, 'bh'), (0.03849779766532436, 'strnlght netcom'), (0.03849779766532436, 'strnlght'), (0.03809476040794367, 'uk'), (0.03800423615679456, 'sternlight writes'), (0.03800423615679456, 'court order'), (0.03781669646336013, 'session'), (0.037510674648264755, 'eff org'), (0.037510674648264755, 'decrypt'), (0.03614831279585895, 'devices'), (0.03602999012267536, 'ncsl nist'), (0.03602999012267536, 'encrypt'), (0.03553642861414556, 'bontchev'), (0.035042867105615765, 'ciphertext'), (0.0347579930729413, 'digital'), (0.034479929128357775, 'sci'), (0.034479929128357775, 'drug'), (0.03420186518377424, 's1'), (0.0340460211335956, 'warrant'), (0.03344899465873347, 'byte'), (0.03308960940544011, 'export'), (0.033068621071496565, 'skipjack'), (0.033068621071496565, 'session key'), (0.033068621071496565, 'escrowed'), (0.03305438945009282, 'tap'), (0.03257505956296676, 'wiretaps'), (0.03206275776659004, 'conversations'), (0.03169928968252246, 'companies'), (0.0314579830719041, 'pgp public'), (0.031421225737938936, 'hardware'), (0.031094375037377366, 'encryption algorithm'), (0.031094375037377366, 'cryptosystem'), (0.030740582188586326, 'amendment'), (0.03010725202031777, 'gtoal'), (0.03010725202031777, 'escrow agencies'), (0.03010725202031777, 'dorothy'), (0.02975284207043775, 'si'), (0.02975284207043775, 'federal'), (0.029613690511787968, 'terrorists'), (0.029613690511787968, 'tempest'), (0.029120129003258166, 'encryption technology'), (0.029120129003258166, 'decryption'), (0.028626567494728368, 'md5'), (0.028626567494728368, 'ciphers'), (0.02813300598619857, 'cryptosystems'), (0.028084458402936573, 'ensure'), (0.02776568713807797, 'holland'), (0.027435143243577043, 'cellular'), (0.02714588296913897, 'drug dealers'), (0.02714588296913897, 'dealers'), (0.02707775758087948, '80 bit'), (0.026679555263513607, 'rwing'), (0.026652321460609172, 'escrow system')]\n",
      "\n",
      "The 100 most typical words for REC.SPORT.BASEBALL are:\n",
      "[(0.1824522497792227, 'braves'), (0.1799298684458694, 'pitcher'), (0.17621213993729345, 'players'), (0.16143240533461178, 'hitter'), (0.15134288000119855, 'clutch'), (0.14377573600113863, 'cubs'), (0.13227768954043045, 'pitching'), (0.12864144800101876, 'batting'), (0.12210248265270501, 'sox'), (0.11014398488976117, 'jays'), (0.10942205463848062, 'player'), (0.10425842844527013, 'alomar'), (0.09971702749970911, 'mets'), (0.09971702749970911, 'fans'), (0.09293355624122548, 'gant'), (0.09080572800071914, 'rbi'), (0.08828334666736583, 'phillies'), (0.08818512636028696, 'pitchers'), (0.0832385840006592, 'yankees'), (0.08239779022287477, 'red sox'), (0.07987540888952147, 'clemens'), (0.07936661372425827, 'giants'), (0.0760170679277593, 'stats'), (0.07567144000059928, 'innings'), (0.07483064622281484, 'reds'), (0.07398985244503041, 'inning'), (0.07230826488946153, 'winfield'), (0.06982308461512705, 'bonds'), (0.06978588355610822, 'runner'), (0.06810429600053935, 'batter'), (0.06558191466718603, 'obp'), (0.06558191466718603, 'hirschbeck'), (0.06390032711161717, 'dodgers'), (0.06137794577826386, 'pitched'), (0.060537152000479426, 'mattingly'), (0.059016199948807434, 'defensive'), (0.05765950569711071, 'pitches'), (0.05717397688934167, 'catcher'), (0.05717397688934167, 'baerga'), (0.05630893920574762, 'hr'), (0.05630893920574762, 'bat'), (0.055492389333772804, 'blue jays'), (0.05381080177820393, '03 03'), (0.0529700080004195, 'sabo'), (0.05223272869032381, 'scored'), (0.05212921422263506, 'singer'), (0.05212921422263506, 'mark singer'), (0.05212921422263506, 'boggs'), (0.05128842044485062, 'twins'), (0.05128842044485062, 'fielder'), (0.05044762666706618, 'world series'), (0.05044762666706618, 'rockies'), (0.05044762666706618, 'orioles'), (0.04973729756294573, 'winning'), (0.04878992046650867, 'hits'), (0.048766039111497315, 'mss netcom'), (0.048162645935233644, 'sports'), (0.048162645935233644, 'lopez'), (0.047925245333712875, 'cardinals'), (0.047084451555928435, 'viola'), (0.047084451555928435, 'fielding'), (0.047084451555928435, 'batters'), (0.046243657778144, 'cincinnati'), (0.04540286400035957, 'valentine'), (0.044053034984323364, 'ab'), (0.043721276444790694, 'sandberg'), (0.043414216054295116, 'umpire'), (0.04335788318842567, 'jewish'), (0.0431056578878863, 'frank'), (0.04273586892844675, 'bullpen'), (0.04263196933966777, 'strike'), (0.04263196933966777, 'nl'), (0.04203968888922182, 'rangers'), (0.04168459224323071, 'williams'), (0.041379174676750036, 'aaa'), (0.04121090369501217, 'philadelphia'), (0.041198895111437386, 'batting average'), (0.04110552562019577, 'stadium'), (0.04110552562019577, 'offense'), (0.04073721514679365, 'wins'), (0.03978983805035659, 'duke edu'), (0.03951730755586851, 'baseman'), (0.039344133299204956, 'mss'), (0.039316149502138055, 'colorado'), (0.039316149502138055, '300'), (0.038665786173356594, 'tigers'), (0.038665786173356594, 'sherri'), (0.03829007865990838, 'pitt edu'), (0.03798743904750823, 'majors'), (0.03798743904750823, 'cs cornell'), (0.03783572000029964, 'infield'), (0.03783572000029964, 'home runs'), (0.03783572000029964, 'astros'), (0.03742139530926394, 'staff'), (0.03716389987579343, 'ba'), (0.036994926222515205, 'steph'), (0.03660081048373595, 'outs'), (0.036154132444730765, 'lankford'), (0.036154132444730765, '02 04'), (0.03531333866694633, 'wip')]\n",
      "\n",
      "The 100 most typical words for ALT.ATHEISM are:\n",
      "[(0.3208952766481447, 'atheism'), (0.1973449651448383, 'atheists'), (0.184042879254083, 'islam'), (0.17901438178193244, 'religion'), (0.14150458915530204, 'atheist'), (0.12505477692905637, 'livesey'), (0.12495558332303205, 'moral'), (0.12009525960530049, 'morality'), (0.11168414040205037, 'theism'), (0.10723138710699914, 'bible'), (0.10279705579891897, 'islamic'), (0.09836928899898269, 'religious'), (0.09748307918818104, 'christian'), (0.0849428673480383, 'alt atheism'), (0.08122236507568906, 'muslim'), (0.08101032719303654, 'jaeger'), (0.08020198787754894, 'uk'), (0.07865080310003546, 'rushdie'), (0.07550477097603404, 'qur'), (0.07487686780415086, 'mathew'), (0.07424231807699704, 'christianity'), (0.07157223082103228, 'muslims'), (0.0676396906660305, 'theists'), (0.06685318263503015, 'vice ico'), (0.06685318263503015, 'ico tek'), (0.06685318263503015, 'ico'), (0.06606667460402979, 'cc monash'), (0.06528016657302944, 'sandvik'), (0.06528016657302944, 'jon livesey'), (0.06478823215549105, 'edu keith'), (0.06472407216968973, 'christians'), (0.06370715051102874, 'benedikt'), (0.062185873261074436, 'cobb'), (0.061347626418027656, 'livesey writes'), (0.06057436339741033, 'sgi com'), (0.058988102325026597, 'solntze wpd'), (0.058988102325026597, 'solntze'), (0.058988102325026597, 'livesey solntze'), (0.05837857489815151, 'gregg'), (0.05820159429402624, 'motto'), (0.057109475443843864, 'fallacy'), (0.05393672680807477, 'monash edu'), (0.05393672680807477, 'monash'), (0.05348254610802411, 'morals'), (0.05330217708092095, 'punishment'), (0.05317258864809874, 'frank'), (0.052696038077023756, 'dwyer'), (0.052696038077023756, 'beauchaine'), (0.05266762735376713, 'religions'), (0.0519095300460234, 'bobbe vice'), (0.0519095300460234, 'bobbe'), (0.05112302201502305, 'khomeini'), (0.05095706412109463, 'sgi'), (0.050566425096968626, 'murder'), (0.05012942844515184, 'satan'), (0.050039691502208536, 'tek com'), (0.050039691502208536, 'tek'), (0.04955000595302234, 'osrhe'), (0.04876349792202199, 'frank dwyer'), (0.047190481860021276, 'mozumder'), (0.046969119972487226, 'gods'), (0.04608291016168558, 'edu au'), (0.0456875803550751, 'existence god'), (0.044830957767020216, 'wwc edu'), (0.044830957767020216, 'wwc'), (0.044830957767020216, 'saturn wwc'), (0.04424562195984755, 'church'), (0.044044449736019856, 'fred rice'), (0.04188028199215217, 'mantis'), (0.041684925643018796, 'yoyo cc'), (0.041684925643018796, 'yoyo'), (0.041684925643018796, 'moral system'), (0.041684925643018796, 'darice yoyo'), (0.041684925643018796, 'darice'), (0.04089841761201844, 'gregg jaeger'), (0.04011190958101809, 'jaeger buphy'), (0.04011190958101809, 'buphy bu'), (0.04011190958101809, 'buphy'), (0.04003175320176683, 'rice'), (0.03997663281069071, 'mantis co'), (0.03987944148607406, 'vice'), (0.03950501960700674, 'wpd sgi'), (0.03950501960700674, 'wpd'), (0.03932540155001773, 'god exist'), (0.03897828601224665, 'liar'), (0.03897828601224665, 'com jon'), (0.038538893519017375, 'verses'), (0.038538893519017375, 'kent sandvik'), (0.03775238548801702, 'kmr4'), (0.03775238548801702, 'cobb alexia'), (0.03696587745701667, 'sandvik newton'), (0.03696587745701667, 'newton apple'), (0.03696587745701667, 'mangoe'), (0.03696587745701667, 'frank d012s658'), (0.03696587745701667, 'fatwa'), (0.03696587745701667, 'd012s658 uucp'), (0.03696587745701667, 'd012s658'), (0.036179369426016315, 'oulu'), (0.036179369426016315, 'edu gregg'), (0.036179369426016315, 'bmd trw')]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_and_preprocess_news_group_texts_separate_groups(list_of_subjects_to_select):\n",
    "    \n",
    "    # Only select a subset of the newsgroup data\n",
    "    data_for_all_subjects = []\n",
    "    for subject in list_of_subjects_to_select:\n",
    "        print(\"Reading from:\", subject)\n",
    "        data_for_subject = []\n",
    "        for file_name in glob.glob(f'/kaggle/input/news-groups-lab/20news-18828/{subject}/*'):\n",
    "            with open(file_name, 'r', encoding='cp1252') as f:\n",
    "                lines = [line.replace(\"_\", \" \").strip() for line in f.readlines() if not (line.startswith(\"From:\") or line.startswith(\"Subject:\"))]\n",
    "                data_for_subject.append(\" \".join(lines))\n",
    "        data_for_all_subjects.append(\" \".join(data_for_subject))\n",
    "    return data_for_all_subjects\n",
    "\n",
    "\n",
    "# Select a subset of the newsgruops to investigate\n",
    "news_group_subjects = ['sci.space', 'sci.med', 'sci.crypt', 'rec.sport.baseball', 'alt.atheism']\n",
    "  \n",
    "# Read the texts\n",
    "text_data = get_and_preprocess_news_group_texts_separate_groups(news_group_subjects)\n",
    "print(f\"Have read texts for {len(text_data)} news groups\\n\")\n",
    "\n",
    "# Count the frequencies of the different words\n",
    "\n",
    "# The line below is the original, changed in task 16 below\n",
    "# vectorizer = CountVectorizer(stop_words = stopwords.words('english'))\n",
    "#\n",
    "# The line below was again changed in task 18 \n",
    "# vectorizer = CountVectorizer(stop_words = stopwords.words('english'), max_df=len(news_group_subjects)-1, ngram_range=(1,2))\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords.words('english') + ['msg', 'n3jxp'], max_df=len(news_group_subjects)-1, ngram_range=(1,2))\n",
    "\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Report the output\n",
    "NR_OF_WORDS_TO_SHOW = 100\n",
    "for transformed, subject in zip(X, news_group_subjects):\n",
    "    \n",
    "    score_vec = transformed.toarray()[0] #Get a vector with scores for the subject, for each of the words in the corpus\n",
    "    scores_with_words = [(score, word) for score, word in zip(score_vec, vectorizer.get_feature_names_out())] \n",
    "        \n",
    "    sorted_scores_with_words = sorted(scores_with_words, reverse=True)\n",
    "    \n",
    "    print(f\"\\nThe {NR_OF_WORDS_TO_SHOW} most typical words for {subject.upper()} are:\")\n",
    "    print(sorted_scores_with_words[:NR_OF_WORDS_TO_SHOW])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee319481",
   "metadata": {
    "papermill": {
     "duration": 0.0108,
     "end_time": "2024-10-02T15:58:41.825309",
     "exception": false,
     "start_time": "2024-10-02T15:58:41.814509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "15) Run the code for extracting the most frequent words. Try to understand the code, and look at the output. Does the most frequent words seem to represent the news group subjects well?\n",
    "\n",
    "> !💬! It represents the subject well enough that it would be easy to 'reverse engineer' and match the subjects to the 'word clouds' \n",
    "\n",
    "16) 💻💻 The [CountVectorizer class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn is used.  Find out from the documentation of the class, how you change the code above, so that words that occur in all four news group classes are excluded from the results. The code above creates one \"document\" from each of the news group classes. That is, you should change the code so that the maximum number of documents that a word can be included in should be **three**, because you have four documents. When you do this, how does the result change?\n",
    "\n",
    "> !💬! It is a major improvement, almost all noise from common words is gone\n",
    "\n",
    "17) 💻💻 Add an additional news group category to the list `news_group_subjects`. (In Kaggle, in the right-hand side panel, you have the data sets, if you expand the new-groups-lab, you will find the names of the news gruops), Run the code again, and see if you get words that seem respresentative to that category.\n",
    "\n",
    "> !💬! Added ```alt.atheism``` and the words seem generally relevant\n",
    "\n",
    "18) 💻💻 In step 17), did you hard-code the maximum number of documents occurrences to **three**? In that case, change the code, so that it uses the length of `news_group_subjects` for determining the maximum number of documents.\n",
    "\n",
    "> !💬! Named argument to countVectorizer: ```max_df=len(news_group_subjects)-1``` \n",
    "\n",
    "17) 💻💻 Change the code, so that in addition to single words, also n-grams are extracted, containing sequences of up to two words.\n",
    "\n",
    "> !💬! Named argument to countVectorizer: ```ngram_range=(1,2)```\n",
    "\n",
    "18) 💻💻 There is another class in Scikit-learn that that instead of, as CountVectorizer, which uses raw word frequencies, uses the TF-IDF measure for extracting typical words. What is the name of this class? Change the code so that this class is used instead. How does the lists of the most typical words change?\n",
    "\n",
    "> !💬! The class is ```TfidfVectorizer```. The difference is that each word gets an associated frequency weight between 0 and 1 based on the number of occurrencies in the text\n",
    "\n",
    "19) 💻💻 Add the words 'msg' and 'n3jxp' to the stop word list used.\n",
    "\n",
    "> !💬! Named argument to TfidfVectorizer: ```stop_words = stopwords.words('english') + ['msg', 'n3jxp']```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557b87d",
   "metadata": {
    "papermill": {
     "duration": 0.010725,
     "end_time": "2024-10-02T15:58:41.848011",
     "exception": false,
     "start_time": "2024-10-02T15:58:41.837286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "V: Small code refactoring and data validation\n",
    "=================================================\n",
    "\n",
    "20) 💻💻 This code line now occurs twice in the code above\n",
    "```\n",
    "lines = [line.replace(\"_\", \" \").strip() for line in f.readlines() if not (line.startswith(\"From:\") or line.startswith(\"Subject:\"))]\n",
    "```\n",
    "\n",
    "Rewrite the code so that you instead use a function for this, so avoid repeating code\n",
    "\n",
    "> !💬! Add a function:\n",
    ">```\n",
    "def clean_lines(lines):\n",
    "    return [line.replace(\"_\", \" \").strip() for line in f.readlines() if not (line.startswith(\"From:\") or line.startswith(\"Subject:\"))]\n",
    ">```\n",
    "\n",
    "> Replace original code with: ```lines = clean_lines(lines)```\n",
    "\n",
    "\n",
    "21) 💻💻 Are the variable names used good, or could some of them be improved? In that case, change them to better variable names.\n",
    "\n",
    "> !💬! OK.\n",
    "\n",
    "22) 💻💻 Add an exception in the code above, that handles the case when `news_group_subjects` contains a news group that does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494d3a3",
   "metadata": {
    "papermill": {
     "duration": 0.0098,
     "end_time": "2024-10-02T15:58:41.868628",
     "exception": false,
     "start_time": "2024-10-02T15:58:41.858828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Extra\n",
    "-----\n",
    "(a) Now, larger text passages than sentences are sent to the word2vec model. So the model learns word contexts over the sentence-border. Try to split the texts into sentences before sending it to the model. Does it change the results in any way? To do that, you can, for instance look at the [tokenize](https://www.nltk.org/api/nltk.tokenize.html) package of the NLTK library.\n",
    "\n",
    "(b) Find out how to use scikit learn to train a text classifyer for the five news groups you used above. That is, a classifier that can determine which new group a text belongs to.\n",
    "\n",
    "(c) There is a separate [Notebook on Topic modelling](https://www.kaggle.com/code/mariaskeppstedt/topic-modelling-example), which forms an example of another type of unsupervised language model. Run it on the same five news group categories you used here. Do you get any sensible topics?\n",
    "\n",
    "(d) Rewrite the code in section iv above, so that is used a dataFrame instead."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4581043,
     "sourceId": 7819069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4742974,
     "sourceId": 8044105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 222.987184,
   "end_time": "2024-10-02T15:58:45.352141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-02T15:55:02.364957",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
